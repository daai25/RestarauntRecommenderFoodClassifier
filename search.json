[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About This Documentation",
    "section": "",
    "text": "Well… currently there is nothing… so I dunno",
    "crumbs": [
      "About This Documentation"
    ]
  },
  {
    "objectID": "project_charta.html",
    "href": "project_charta.html",
    "title": "Room for new Information again!!! AHHHHH",
    "section": "",
    "text": "Room for new Information again!!! AHHHHH",
    "crumbs": [
      "Room for new Information again!!! AHHHHH"
    ]
  },
  {
    "objectID": "md-templates/data_report.html",
    "href": "md-templates/data_report.html",
    "title": "Sample Project - Data Report",
    "section": "",
    "text": "All information on the data used in the project is compiled in the data report in order to ensure the traceability and reproducibility of the results and to enable a systematic expansion of the database.\nTypically, in the exploratory analysis of the acquired raw data, quality and other issues are identified, which require pre-processing, merging of individual datasets and feature engineering into processed datasets. Therefore, this template provides a separate section for the processed data, which then serves as a starting point for the modelling activities. This needs to be adapted to the specific project requirements.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nQuelle\nStorage location\n\n\n\n\nDataset 1\nName/short description of the data source\nLink and/or short description of the location where the data is stored, e.g. accessible to the team\n\n\nDataset 2\n…\n…\n\n\n\n\n\n\n\nDescription of what information the dataset contains\nDetails of the data source/provider\nInformation on data procurement: description and possibly references to resources (download scripts, tools, online services, …). Any new team member should be able to acquire the data indepentendently following these instructions.\nLegal aspects of data use, licences, etc.\nData governance aspects: Categorisation of the data based on internal business requirements, e.g. public, business-relevant, personal\nIf applicable: categorisation into dependent (target variable, regressor) and independent (regressor) variables\n…\n\n\n\nThe data catalogue basically represents an extended schema of a relational database.\n\n\n\n\n\n\n\n\n\n\nColumn index\nColumn name\nDatatype\nValues (Range, validation rules)\nShort description\n\n\n\n\n1\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nQuelle\nStorage location\n\n\n\n\nProcessed Dataset 1\nName/short description of the data source\nLink and/or short description of the location where the data is stored, e.g. accessible to the team\n\n\nProcessed Dataset 2\n…\n…\n\n\n\n\n\n\n\nDescription of what information the dataset contains\nDetails and reasons for the processing steps -&gt; Traceability and ensuring reproducibility\nHow can the data be accessed? Description, scripts, tools, …\n…\n\n\n\n\n\n\n\n\n\n\n…"
  },
  {
    "objectID": "md-templates/data_report.html#raw-data",
    "href": "md-templates/data_report.html#raw-data",
    "title": "Sample Project - Data Report",
    "section": "",
    "text": "Name\nQuelle\nStorage location\n\n\n\n\nDataset 1\nName/short description of the data source\nLink and/or short description of the location where the data is stored, e.g. accessible to the team\n\n\nDataset 2\n…\n…\n\n\n\n\n\n\n\nDescription of what information the dataset contains\nDetails of the data source/provider\nInformation on data procurement: description and possibly references to resources (download scripts, tools, online services, …). Any new team member should be able to acquire the data indepentendently following these instructions.\nLegal aspects of data use, licences, etc.\nData governance aspects: Categorisation of the data based on internal business requirements, e.g. public, business-relevant, personal\nIf applicable: categorisation into dependent (target variable, regressor) and independent (regressor) variables\n…\n\n\n\nThe data catalogue basically represents an extended schema of a relational database.\n\n\n\n\n\n\n\n\n\n\nColumn index\nColumn name\nDatatype\nValues (Range, validation rules)\nShort description\n\n\n\n\n1\n\n\n\n\n\n\n2"
  },
  {
    "objectID": "md-templates/data_report.html#processed-data",
    "href": "md-templates/data_report.html#processed-data",
    "title": "Sample Project - Data Report",
    "section": "",
    "text": "Name\nQuelle\nStorage location\n\n\n\n\nProcessed Dataset 1\nName/short description of the data source\nLink and/or short description of the location where the data is stored, e.g. accessible to the team\n\n\nProcessed Dataset 2\n…\n…\n\n\n\n\n\n\n\nDescription of what information the dataset contains\nDetails and reasons for the processing steps -&gt; Traceability and ensuring reproducibility\nHow can the data be accessed? Description, scripts, tools, …\n…\n\n\n\n\n\n\n\n\n\n\n…"
  },
  {
    "objectID": "md-templates/modelling_report.html",
    "href": "md-templates/modelling_report.html",
    "title": "Sample Project - Modelling Report",
    "section": "",
    "text": "The report should summarise the details of the modelling activities, e.g. machine learning experiments.\n\n\n\nAim of the modelling - consistent with Data Mining Goals in the project charta\nData set(s) and/or feature set used (references to the data report)\nDescription of the independent variables and (if applicable) the target variable\nType of model used or developed\n\n\n\n\nOverview of the models used and/or implemented and their configurations\n\nDetailed description of the model used (e.g. literature references, specification of the software library, exact module, version or other implementation details etc.)\nGraphical representation of the modelling pipeline\nIf applicable: link to the code of the modelling pipeline, version information in code repository, configuration files\nIf possible, links to the artefacts of the executed modelling pipeline (training experiment)\nLink to the literature in which the model/method is described\nHyperparameters\n\n\n\n\nKey figures dependent on the model and modelling objective\n\nRMSD, ROC/Lift-Charts, AUC, Confusion Matrix, Accuracy, Precision, Recall\nCoherence, Perplexity, …\nIf applicable: analyses/plots of (hyper)parameter screenings\n\n\n\n\n\nIf applicable: Results from the application of “explanatory models”\nWere the modelling objectives achieved?\nThe findings resulting from the modelling phase: can the project objective be achieved with the results from the modelling phase?\nHow can the findings be used? Are there any limitations?\n\n\n\n\n\nConclusions of the key findings from the modelling phase\nDiscussion about limitations\nProposal for extensions and further work\nProposal for the deployment of the generated insights/model"
  },
  {
    "objectID": "md-templates/modelling_report.html#initial-situation",
    "href": "md-templates/modelling_report.html#initial-situation",
    "title": "Sample Project - Modelling Report",
    "section": "",
    "text": "Aim of the modelling - consistent with Data Mining Goals in the project charta\nData set(s) and/or feature set used (references to the data report)\nDescription of the independent variables and (if applicable) the target variable\nType of model used or developed"
  },
  {
    "objectID": "md-templates/modelling_report.html#model-descriptions",
    "href": "md-templates/modelling_report.html#model-descriptions",
    "title": "Sample Project - Modelling Report",
    "section": "",
    "text": "Overview of the models used and/or implemented and their configurations\n\nDetailed description of the model used (e.g. literature references, specification of the software library, exact module, version or other implementation details etc.)\nGraphical representation of the modelling pipeline\nIf applicable: link to the code of the modelling pipeline, version information in code repository, configuration files\nIf possible, links to the artefacts of the executed modelling pipeline (training experiment)\nLink to the literature in which the model/method is described\nHyperparameters"
  },
  {
    "objectID": "md-templates/modelling_report.html#results",
    "href": "md-templates/modelling_report.html#results",
    "title": "Sample Project - Modelling Report",
    "section": "",
    "text": "Key figures dependent on the model and modelling objective\n\nRMSD, ROC/Lift-Charts, AUC, Confusion Matrix, Accuracy, Precision, Recall\nCoherence, Perplexity, …\nIf applicable: analyses/plots of (hyper)parameter screenings"
  },
  {
    "objectID": "md-templates/modelling_report.html#model-interpretation",
    "href": "md-templates/modelling_report.html#model-interpretation",
    "title": "Sample Project - Modelling Report",
    "section": "",
    "text": "If applicable: Results from the application of “explanatory models”\nWere the modelling objectives achieved?\nThe findings resulting from the modelling phase: can the project objective be achieved with the results from the modelling phase?\nHow can the findings be used? Are there any limitations?"
  },
  {
    "objectID": "md-templates/modelling_report.html#conclusions-and-next-steps",
    "href": "md-templates/modelling_report.html#conclusions-and-next-steps",
    "title": "Sample Project - Modelling Report",
    "section": "",
    "text": "Conclusions of the key findings from the modelling phase\nDiscussion about limitations\nProposal for extensions and further work\nProposal for the deployment of the generated insights/model"
  },
  {
    "objectID": "data_report.html",
    "href": "data_report.html",
    "title": "Room for new Information",
    "section": "",
    "text": "For extracting the data from OpenStreetMap, we utilized the Overpass API. The used query is as follow:\n[out:json][timeout:180];\narea[\"name\"=\"{name}\"][\"boundary\"=\"administrative\"][\"admin_level\"=\"4\"]-&gt;.a;\n(\n  node[\"amenity\"=\"restaurant\"](area.a);\n  node[\"amenity\"=\"cafe\"](area.a);\n  node[\"amenity\"=\"fast_food\"](area.a);\n);\nout center;\n\n\n\n\n\n[out:json][timeout:180];\n\nout:json: Sets the output format to JSON.\ntimeout:180: Sets a timeout limit of 180 seconds for the query to run, useful for large or slow queries.\n\n\n\n\n\narea[\"name\"=\"{name}\"][\"boundary\"=\"administrative\"][\"admin_level\"=\"4\"]-&gt;.a;\n\nThis finds an administrative area:\n\nWith name {name} (e.g. “Germany” or “Zurich” – replace with the actual name).\nWith boundary=administrative (only administrative boundaries).\nWith admin_level=4 (typically a region/state-level boundary).\n\n-&gt;.a;: Saves the matched area into a variable .a.\n\nNote: The area is not the same as a polygon in the map. Internally, Overpass assigns IDs to areas derived from OSM relations.\n\n\n\n\n(\n  node[\"amenity\"=\"restaurant\"](area.a);\n  node[\"amenity\"=\"cafe\"](area.a);\n  node[\"amenity\"=\"fast_food\"](area.a);\n);\n\nThis block fetches nodes (points) that:\n\nHave amenity=restaurant, amenity=cafe, or amenity=fast_food.\nAre located within the area .a defined above.\n\nParentheses group the different queries together so the result includes all three types.\n\n\n\n\n\nout center;\n\nout center: Outputs each matching object with its center coordinates.\n\ncenter is typically used for areas (ways/relations), but if only nodes are returned, the output is similar to out body.\n\n\n\n\n\n\nFor our example we extracted all the restaurants from all the cantons in Switzerland. This gives us the following list of JSON files:\ncantons/\n├── restaurants_Aargau.json\n├── restaurants_Appenzell_Ausserrhoden.json\n├── restaurants_Appenzell_Innerrhoden.json\n├── restaurants_Basel-Landschaft.json\n├── restaurants_Basel-Stadt.json\n├── restaurants_Bern_Berne.json\n├── restaurants_Fribourg_Freiburg.json\n├── restaurants_Genève.json\n├── restaurants_Glarus.json\n├── restaurants_Graubünden_Grischun_Grigioni.json\n├── restaurants_Jura.json\n├── restaurants_Luzern.json\n├── restaurants_Neuchâtel.json\n├── restaurants_Nidwalden.json\n├── restaurants_Obwalden.json\n├── restaurants_Schaffhausen.json\n├── restaurants_Schwyz.json\n├── restaurants_Solothurn.json\n├── restaurants_St._Gallen.json\n├── restaurants_Thurgau.json\n├── restaurants_Ticino.json\n├── restaurants_Uri.json\n├── restaurants_Valais_Wallis.json\n├── restaurants_Vaud.json\n├── restaurants_Zug.json\n└── restaurants_Zürich.json\nAfter combing all the JSON into one single JSON file, the data can be analyzed. The following statistics were generated:\nRestaurant Statistics – restaurants_Switzerland.json\n\nTotal gathered: 20977\n- Restaurants: 14917\n- Fast food: 2710\n- Cafes: 3350\n\nAdditional information:\n- With URL: 8924 (42.54%)\n- With cuisine type: 9440 (45.00%)\n- With URL and cuisine type: 5064 (24.14%)\n\n\nBased on the analysis it is visible that only 50% percent of all the restaurants e.g. have an URL or a cuisine type. And only 28% have both. Even with the missing data, the dataset can still be used to test or verify the FoodClassifier.\n\n\n\nFor the next steps, the cuisine types needs to be extracted from the JSON file. The FoodClassifier will use these cuisine types as labels for the training data and also will be used as result of the module.\nThe simplest way to extract the cuisine types is to use a set, which will automatically remove duplicates. The cuisine types are stored in the cuisines set.\nimport json\n\nwith open(\"restaurants_Switzerland.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# prepare set for the cuisine type\ncuisines = set()\n\nfor element in data.get(\"elements\", []):\n    tags = element.get(\"tags\", {})\n    cuisine = tags.get(\"cuisine\")\n    if cuisine:\n        # split multiple types\n        types = [c.strip() for c in cuisine.split(\";\")]\n        cuisines.update(types)\nNow we take a look at the extracted cuisine types:\nCentral American\nGourmet\nGrill\nPains\nPizza & Grill\nSchnitzel\nSouthern_BBQ\nTexas_Barbecue\nafghan\nafrican\namerican\n...\nhomemade\nhot_dog\nhttps://labelfaitmaison.ch/de/restaurant/roba-buona-2/\nice_cream\nindian\n...\nWe see the cuisine types are not really normalized. For example: * Uppercase vs. lowercase * Spaces vs. underscores * Types seperated with ‘&’ * URLs in the cuisine type\nNow we need to update the python code to normalize the cuisine types, with the following rules: * Convert to lowercase * Replace spaces with underscores * Additionally, split types separated by ‘&’ and add them as separate entries * Remove URLs from the cuisine type\nThis leads to the following updated code:\nimport re\nimport json\n\nwith open(\"restaurants_Switzerland.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# prepare set for the cuisine type\ncuisines = set()\n\nfor element in data.get(\"elements\", []):\n    tags = element.get(\"tags\", {})\n    cuisine = tags.get(\"cuisine\")\n    if cuisine:\n        # split multiple types\n        types = [c.strip() for c in re.split(r'[;&]', cuisine)]\n\n        cleaned_types = [\n            t.replace(\" \", \"_\").replace(\"-\", \"_\").lower() # clean up the types\n            for t in types\n            if not t.strip().lower().startswith(\"http\") # filter out URLs\n        ]\n        cuisines.update(cleaned_types)\nThis leads to the following normalized cuisine types with 386 unique entries:\nafghan\nafrican\nalp\nalpine_hut\namerican\narab\nargentinian\nasian\naustrian\nayran\nbacon\nbagel\nbakery\nbaklawa\nbalkan\nbangladeshi\nbar\nbar_and_grill\n...\nWe can still see some cuisine types doesn’t really match our expectations, like “alpine_hut” isn’t really a cuisine type and could be fused with “alp”.",
    "crumbs": [
      "Room for new Information"
    ]
  },
  {
    "objectID": "data_report.html#extracting-restaurants-fast-food-and-cafes-from-openstreetmap",
    "href": "data_report.html#extracting-restaurants-fast-food-and-cafes-from-openstreetmap",
    "title": "Room for new Information",
    "section": "",
    "text": "For extracting the data from OpenStreetMap, we utilized the Overpass API. The used query is as follow:\n[out:json][timeout:180];\narea[\"name\"=\"{name}\"][\"boundary\"=\"administrative\"][\"admin_level\"=\"4\"]-&gt;.a;\n(\n  node[\"amenity\"=\"restaurant\"](area.a);\n  node[\"amenity\"=\"cafe\"](area.a);\n  node[\"amenity\"=\"fast_food\"](area.a);\n);\nout center;\n\n\n\n\n\n[out:json][timeout:180];\n\nout:json: Sets the output format to JSON.\ntimeout:180: Sets a timeout limit of 180 seconds for the query to run, useful for large or slow queries.\n\n\n\n\n\narea[\"name\"=\"{name}\"][\"boundary\"=\"administrative\"][\"admin_level\"=\"4\"]-&gt;.a;\n\nThis finds an administrative area:\n\nWith name {name} (e.g. “Germany” or “Zurich” – replace with the actual name).\nWith boundary=administrative (only administrative boundaries).\nWith admin_level=4 (typically a region/state-level boundary).\n\n-&gt;.a;: Saves the matched area into a variable .a.\n\nNote: The area is not the same as a polygon in the map. Internally, Overpass assigns IDs to areas derived from OSM relations.\n\n\n\n\n(\n  node[\"amenity\"=\"restaurant\"](area.a);\n  node[\"amenity\"=\"cafe\"](area.a);\n  node[\"amenity\"=\"fast_food\"](area.a);\n);\n\nThis block fetches nodes (points) that:\n\nHave amenity=restaurant, amenity=cafe, or amenity=fast_food.\nAre located within the area .a defined above.\n\nParentheses group the different queries together so the result includes all three types.\n\n\n\n\n\nout center;\n\nout center: Outputs each matching object with its center coordinates.\n\ncenter is typically used for areas (ways/relations), but if only nodes are returned, the output is similar to out body.\n\n\n\n\n\n\nFor our example we extracted all the restaurants from all the cantons in Switzerland. This gives us the following list of JSON files:\ncantons/\n├── restaurants_Aargau.json\n├── restaurants_Appenzell_Ausserrhoden.json\n├── restaurants_Appenzell_Innerrhoden.json\n├── restaurants_Basel-Landschaft.json\n├── restaurants_Basel-Stadt.json\n├── restaurants_Bern_Berne.json\n├── restaurants_Fribourg_Freiburg.json\n├── restaurants_Genève.json\n├── restaurants_Glarus.json\n├── restaurants_Graubünden_Grischun_Grigioni.json\n├── restaurants_Jura.json\n├── restaurants_Luzern.json\n├── restaurants_Neuchâtel.json\n├── restaurants_Nidwalden.json\n├── restaurants_Obwalden.json\n├── restaurants_Schaffhausen.json\n├── restaurants_Schwyz.json\n├── restaurants_Solothurn.json\n├── restaurants_St._Gallen.json\n├── restaurants_Thurgau.json\n├── restaurants_Ticino.json\n├── restaurants_Uri.json\n├── restaurants_Valais_Wallis.json\n├── restaurants_Vaud.json\n├── restaurants_Zug.json\n└── restaurants_Zürich.json\nAfter combing all the JSON into one single JSON file, the data can be analyzed. The following statistics were generated:\nRestaurant Statistics – restaurants_Switzerland.json\n\nTotal gathered: 20977\n- Restaurants: 14917\n- Fast food: 2710\n- Cafes: 3350\n\nAdditional information:\n- With URL: 8924 (42.54%)\n- With cuisine type: 9440 (45.00%)\n- With URL and cuisine type: 5064 (24.14%)\n\n\nBased on the analysis it is visible that only 50% percent of all the restaurants e.g. have an URL or a cuisine type. And only 28% have both. Even with the missing data, the dataset can still be used to test or verify the FoodClassifier.\n\n\n\nFor the next steps, the cuisine types needs to be extracted from the JSON file. The FoodClassifier will use these cuisine types as labels for the training data and also will be used as result of the module.\nThe simplest way to extract the cuisine types is to use a set, which will automatically remove duplicates. The cuisine types are stored in the cuisines set.\nimport json\n\nwith open(\"restaurants_Switzerland.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# prepare set for the cuisine type\ncuisines = set()\n\nfor element in data.get(\"elements\", []):\n    tags = element.get(\"tags\", {})\n    cuisine = tags.get(\"cuisine\")\n    if cuisine:\n        # split multiple types\n        types = [c.strip() for c in cuisine.split(\";\")]\n        cuisines.update(types)\nNow we take a look at the extracted cuisine types:\nCentral American\nGourmet\nGrill\nPains\nPizza & Grill\nSchnitzel\nSouthern_BBQ\nTexas_Barbecue\nafghan\nafrican\namerican\n...\nhomemade\nhot_dog\nhttps://labelfaitmaison.ch/de/restaurant/roba-buona-2/\nice_cream\nindian\n...\nWe see the cuisine types are not really normalized. For example: * Uppercase vs. lowercase * Spaces vs. underscores * Types seperated with ‘&’ * URLs in the cuisine type\nNow we need to update the python code to normalize the cuisine types, with the following rules: * Convert to lowercase * Replace spaces with underscores * Additionally, split types separated by ‘&’ and add them as separate entries * Remove URLs from the cuisine type\nThis leads to the following updated code:\nimport re\nimport json\n\nwith open(\"restaurants_Switzerland.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# prepare set for the cuisine type\ncuisines = set()\n\nfor element in data.get(\"elements\", []):\n    tags = element.get(\"tags\", {})\n    cuisine = tags.get(\"cuisine\")\n    if cuisine:\n        # split multiple types\n        types = [c.strip() for c in re.split(r'[;&]', cuisine)]\n\n        cleaned_types = [\n            t.replace(\" \", \"_\").replace(\"-\", \"_\").lower() # clean up the types\n            for t in types\n            if not t.strip().lower().startswith(\"http\") # filter out URLs\n        ]\n        cuisines.update(cleaned_types)\nThis leads to the following normalized cuisine types with 386 unique entries:\nafghan\nafrican\nalp\nalpine_hut\namerican\narab\nargentinian\nasian\naustrian\nayran\nbacon\nbagel\nbakery\nbaklawa\nbalkan\nbangladeshi\nbar\nbar_and_grill\n...\nWe can still see some cuisine types doesn’t really match our expectations, like “alpine_hut” isn’t really a cuisine type and could be fused with “alp”.",
    "crumbs": [
      "Room for new Information"
    ]
  },
  {
    "objectID": "md-templates/evaluation_decision_log.html",
    "href": "md-templates/evaluation_decision_log.html",
    "title": "Sample Project - Decision Log",
    "section": "",
    "text": "Sample Project - Decision Log\nThis protocol summarises the decisions from the evaluation of the data mining phase, which are made, for example, in a workshop together with the client and other stakeholders.\nDecide:\n\nDo results meet user needs?\nContinuation of the project yes/no\nPlanning of the deployment\nCarry out an additional data mining iteration\n\nAcquire more or different data\nImprovements in the modelling\nAddress governance issues\nor ethical considerations\n…\n\n\nAny presentation material created for such a workshop should also be stored in the docs folder.\nIt is important to list who was involved in the decisions and when they were made.\nThe structure and level of detail of this protocol should be tailored to the customs of the relevant organisation and the requirements of the decision-makers. It might include already a high level description of the envisioned product to be deployed."
  },
  {
    "objectID": "md-templates/project_charta_template.html",
    "href": "md-templates/project_charta_template.html",
    "title": "Sample Project - Project Charta",
    "section": "",
    "text": "Formulate the problem and important information about the domain and/or business area in which the product is to be developed: What exactly is the problem and the expected benefit of the project? Why should we undertake this effort?\nThis includes a summary of the most important findings from the user analysis: relevant segments and user groups. Describe the problems and needs of the users of the product to be developed.\nStakeholders: List the people involved in and affected by the project. Describe their goals and relationships with each other. Visualisation in the form of a stakeholder map can provide a quick overview.\nYou can reference more detailed analyses such as individual “personas” or interviews in separate documents in the appendix.\n\n\n\nDescribe the available resources (personnel, material, (software) tools, infrastructure, etc.) and time as well as restrictions and constraints. Possible risks that may arise during the project are also identified.\n\n\n\nWhen is the project successful from a client/stakeholder perspective: Formulate (qualitative) objectives, wherever possible, corresponding key metrics and the target values to be achieved within the project.\nIt is also often helpful to specify what is explicitly excluded from the project objectives (out of scope).\n\n\n\nMap the problem definition, datasets to be used and primary objective onto a data mining task, e.g.:\n\nClassification\nRegression\nClustering\nOutlier Detection\nAssociation rule learning (market basket analysis)\nRecommender System\nVisualisation\n…\n\nAlong with the definition of the actual technical problem (category) to be solved, the project goals must be mapped onto quitable quantitative metrics and corresponding target values. For example, for a classification task one might specify an F-score of 0.9 as a minimal requirement for an acceptable solution.\nSuch a requirement should be aligned with the overall project goals and/or literature references or justified by other references, respectively.\n\n\n\nDivide the project into individual phases, describe them briefly and draw up a preliminary timetable, e.g. as a Gantt chart:\ngantt\n    title A Gantt Diagram\n    dateFormat YYYY-MM-DD\n    tickInterval 5day\n    section Project Understanding\n        Define problem,     :a1, 2024-07-01, 1d\n        Determine project goals     :a2, 2024-07-01, 1d\n        List available resources     :a3, 2024-07-02, 1d\n        Set data mining goals    :a4, 2024-07-03, 1d\n        Create project plan    :a5, 2024-07-03, 1d\n        Project checkpoint: milestone, m1, 2024-07-04, 4m\n    section Data Acquisition and Exploration\n        Acquire data :a6, 2024-07-02, 2d\n        Exploratory data analysis   :a7, 2024-07-03, 2d\n        \n    section Modelling\n        Create initial model   :a8, 2024-07-09, 1d\n        Additional feature engineering :a9, 2024-07-10, 1d\n        Prepare modelling report :a10, 2024-07-10, 2h\n    section Evaluation\n        Prepare presentation :a10, 2024-07-10, 2h\n        Project presentation : milestone, m2, 2024-07-11, 4m\nSee Mermaid syntax for Gantt charts.\n\n\n\nList the people involved in the development work here with their role titles, tasks and contact details"
  },
  {
    "objectID": "md-templates/project_charta_template.html#problem-definition",
    "href": "md-templates/project_charta_template.html#problem-definition",
    "title": "Sample Project - Project Charta",
    "section": "",
    "text": "Formulate the problem and important information about the domain and/or business area in which the product is to be developed: What exactly is the problem and the expected benefit of the project? Why should we undertake this effort?\nThis includes a summary of the most important findings from the user analysis: relevant segments and user groups. Describe the problems and needs of the users of the product to be developed.\nStakeholders: List the people involved in and affected by the project. Describe their goals and relationships with each other. Visualisation in the form of a stakeholder map can provide a quick overview.\nYou can reference more detailed analyses such as individual “personas” or interviews in separate documents in the appendix."
  },
  {
    "objectID": "md-templates/project_charta_template.html#situation-assessment",
    "href": "md-templates/project_charta_template.html#situation-assessment",
    "title": "Sample Project - Project Charta",
    "section": "",
    "text": "Describe the available resources (personnel, material, (software) tools, infrastructure, etc.) and time as well as restrictions and constraints. Possible risks that may arise during the project are also identified."
  },
  {
    "objectID": "md-templates/project_charta_template.html#project-goals-and-success-criteria",
    "href": "md-templates/project_charta_template.html#project-goals-and-success-criteria",
    "title": "Sample Project - Project Charta",
    "section": "",
    "text": "When is the project successful from a client/stakeholder perspective: Formulate (qualitative) objectives, wherever possible, corresponding key metrics and the target values to be achieved within the project.\nIt is also often helpful to specify what is explicitly excluded from the project objectives (out of scope)."
  },
  {
    "objectID": "md-templates/project_charta_template.html#data-mining-goals",
    "href": "md-templates/project_charta_template.html#data-mining-goals",
    "title": "Sample Project - Project Charta",
    "section": "",
    "text": "Map the problem definition, datasets to be used and primary objective onto a data mining task, e.g.:\n\nClassification\nRegression\nClustering\nOutlier Detection\nAssociation rule learning (market basket analysis)\nRecommender System\nVisualisation\n…\n\nAlong with the definition of the actual technical problem (category) to be solved, the project goals must be mapped onto quitable quantitative metrics and corresponding target values. For example, for a classification task one might specify an F-score of 0.9 as a minimal requirement for an acceptable solution.\nSuch a requirement should be aligned with the overall project goals and/or literature references or justified by other references, respectively."
  },
  {
    "objectID": "md-templates/project_charta_template.html#project-plan",
    "href": "md-templates/project_charta_template.html#project-plan",
    "title": "Sample Project - Project Charta",
    "section": "",
    "text": "Divide the project into individual phases, describe them briefly and draw up a preliminary timetable, e.g. as a Gantt chart:\ngantt\n    title A Gantt Diagram\n    dateFormat YYYY-MM-DD\n    tickInterval 5day\n    section Project Understanding\n        Define problem,     :a1, 2024-07-01, 1d\n        Determine project goals     :a2, 2024-07-01, 1d\n        List available resources     :a3, 2024-07-02, 1d\n        Set data mining goals    :a4, 2024-07-03, 1d\n        Create project plan    :a5, 2024-07-03, 1d\n        Project checkpoint: milestone, m1, 2024-07-04, 4m\n    section Data Acquisition and Exploration\n        Acquire data :a6, 2024-07-02, 2d\n        Exploratory data analysis   :a7, 2024-07-03, 2d\n        \n    section Modelling\n        Create initial model   :a8, 2024-07-09, 1d\n        Additional feature engineering :a9, 2024-07-10, 1d\n        Prepare modelling report :a10, 2024-07-10, 2h\n    section Evaluation\n        Prepare presentation :a10, 2024-07-10, 2h\n        Project presentation : milestone, m2, 2024-07-11, 4m\nSee Mermaid syntax for Gantt charts."
  },
  {
    "objectID": "md-templates/project_charta_template.html#roles-and-contact-details",
    "href": "md-templates/project_charta_template.html#roles-and-contact-details",
    "title": "Sample Project - Project Charta",
    "section": "",
    "text": "List the people involved in the development work here with their role titles, tasks and contact details"
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "Room for new Information again",
    "section": "",
    "text": "Room for new Information again",
    "crumbs": [
      "Room for new Information again"
    ]
  },
  {
    "objectID": "modelling_report.html",
    "href": "modelling_report.html",
    "title": "Room for new Information again… yaay!",
    "section": "",
    "text": "Room for new Information again… yaay!",
    "crumbs": [
      "Room for new Information again... yaay!"
    ]
  }
]